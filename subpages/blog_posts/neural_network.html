<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" type="text/css" href="../../styles/styles.css">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap');
    </style>
    <meta content="width=device-width, initial-scale=1" name="viewport" />
</head>
<body>
    <a href="../blog.html" class="back-button">&#60back</a>
    <div class="content">
        <div class="line" id="heading_home">Building a Neural Network from scratch</div>
        <div class="line" id="body">
            [2023-07-03] 
            <p>I'm trying to improve my knowledge on backpropogation. I feel like my current understanding relies too much on "black-box" abstractions. </p>
            <p>So the next best step? Build a simple NN from scratch. You can follow my progress <a href="https://github.com/sh2002vk/infancy">here.</a> </p>
            <p>Update: I was going to implement it in Cpp but I think to improve the probability I actually complete the
            project, it's better for me to start with python and focus more on how nets work instead of debugging memory dumps all the time.</p>

            [2023-07-08]
            <div class="line" id="subheading_home">Graph VS Matrix Implementation</div>
            <p>You can express the layered model of a neural network through graphs (nodes and weighted edges that are inter-connected) or represent it as a matrix. 
            Matrix implementaions require less compute by leveraging matrix operations, but limit the modifications you can make to how the backpropogation adjusts weights, etc.
            For now I'll be using the matrix implementaion. </p>

            <div class="line" id="subheading_home">Forward propogation</div>
            <p>This part is not too complex, if you take the time to draw it out, you'll see its just matrix multiplication and a few non-linear functions (sigmoid and softmax). One interesting part is the 
                decision on the range of initial randomized weights. Even though weights will be updated through backpropogation, the goal is to have the smoothest possible gradient descent. Having too large a 
                range for possible starting values might lead to exploding gradients. For now I'll set my range for initial values between 0 and 1, but I'm gonna look into
                <a href="https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/">He and Golort initializion</a> once the initial implementaion is complete. 
            </p>

            <p>Initially was using a sigmoid because thats what 3blue1brown used to explain neural nets, but switching to ReLU because the range is too small, and there's no significant gradient. 
            Got forward prop working today, going to complete backward propogation tomorrow.
            </p>

        </div>
    </div>
</body>
</html>
